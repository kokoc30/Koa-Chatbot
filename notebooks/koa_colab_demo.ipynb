{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Koa-Chatbot — Colab Demo (FastAPI + ML Inference)\n",
        "\n",
        "This notebook starts the Koa FastAPI backend and sends requests to `/chat` to verify the ML inference pipeline is running."
      ],
      "metadata": {
        "id": "bG26_02JHKLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kokoc30/Koa-Chatbot.git\n",
        "%cd Koa-Chatbot\n",
        "!ls\n"
      ],
      "metadata": {
        "id": "FF4uGB8b3iy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "req_path = Path(\"requirements.txt\")\n",
        "reqs = req_path.read_text().splitlines() if req_path.exists() else []\n",
        "\n",
        "blocked = (\"torch\", \"torchvision\", \"torchaudio\")\n",
        "filtered = [\n",
        "    r.strip() for r in reqs\n",
        "    if r.strip() and not r.strip().startswith(\"#\")\n",
        "    and not any(r.strip().startswith(b) for b in blocked)\n",
        "]\n",
        "\n",
        "Path(\"requirements_colab.txt\").write_text(\"\\n\".join(filtered) + (\"\\n\" if filtered else \"\"))\n",
        "print(\"Installing (filtered):\", len(filtered), \"packages\")\n",
        "\n",
        "!pip -q install -r requirements_colab.txt\n",
        "!pip -q install accelerate sentencepiece huggingface_hub\n"
      ],
      "metadata": {
        "id": "YzWfZLCI3oGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "!nvidia-smi -L\n"
      ],
      "metadata": {
        "id": "rTYZiya23pa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Hide progress bars/log spam\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
        "\n",
        "hf_token = getpass.getpass(\"Hugging Face token (hidden, optional): \").strip()\n",
        "if hf_token:\n",
        "    login(hf_token)\n",
        "\n",
        "koa_model_id = getpass.getpass(\"Model ID (hidden): \").strip()\n",
        "if not koa_model_id:\n",
        "    raise ValueError(\"Model ID is required to run the demo.\")\n",
        "\n",
        "os.environ[\"KOA_MODEL_ID\"] = koa_model_id\n"
      ],
      "metadata": {
        "id": "t9wjaLzk3qUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "chat_path = Path(\"inference/chat.py\")\n",
        "txt = chat_path.read_text()\n",
        "\n",
        "# 1) Make model configurable via env (no hardcoded model id)\n",
        "import_line = \"import os\\n\"\n",
        "if import_line not in txt:\n",
        "    txt = \"import os\\n\" + txt\n",
        "\n",
        "# Replace the BASE_MODEL_NAME line (your file currently hardcodes it)\n",
        "# Works even if you later change the string.\n",
        "import re\n",
        "txt = re.sub(\n",
        "    r'^BASE_MODEL_NAME\\s*=\\s*\".*?\"\\s*$',\n",
        "    'BASE_MODEL_NAME = os.environ.get(\"KOA_MODEL_ID\")\\n'\n",
        "    'if not BASE_MODEL_NAME:\\n'\n",
        "    '    raise ValueError(\"KOA_MODEL_ID is not set\")\\n',\n",
        "    txt,\n",
        "    flags=re.MULTILINE\n",
        ")\n",
        "\n",
        "# 2) Reduce logging that would reveal model id\n",
        "txt = txt.replace('print(f\"[chat] Loading tokenizer {BASE_MODEL_NAME}...\")', 'print(\"[chat] Loading tokenizer...\")')\n",
        "txt = txt.replace('print(f\"[chat] Loading base model {BASE_MODEL_NAME}...\")', 'print(\"[chat] Loading base model...\")')\n",
        "\n",
        "# 3) Make dtype Colab-safe\n",
        "txt = txt.replace(\"torch_dtype=torch.bfloat16\", \"torch_dtype=torch.float16\")\n",
        "\n",
        "# 4) Disable transformers logs inside runtime (extra safe)\n",
        "if \"from transformers import AutoTokenizer, AutoModelForCausalLM\" in txt and \"transformers.utils import logging\" not in txt:\n",
        "    txt = txt.replace(\n",
        "        \"from transformers import AutoTokenizer, AutoModelForCausalLM\",\n",
        "        \"from transformers import AutoTokenizer, AutoModelForCausalLM\\nfrom transformers.utils import logging as hf_logging\\nhf_logging.set_verbosity_error()\"\n",
        "    )\n",
        "\n",
        "chat_path.write_text(txt)\n",
        "print(\"Patched inference/chat.py for private model selection + quieter logs.\")\n"
      ],
      "metadata": {
        "id": "MduzN3hh3tn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -n '1,120p' inference/api_server.py\n"
      ],
      "metadata": {
        "id": "No4ovK4J3vy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PORT = 9010\n",
        "!nohup python -m uvicorn inference.api_server:app --host 127.0.0.1 --port 9010 --log-level warning > uvicorn.log 2>&1 &\n",
        "print(\"Server started in background. Logs -> uvicorn.log\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zDp1C2qr3w-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time, requests\n",
        "for _ in range(45):\n",
        "    try:\n",
        "        r = requests.get(f\"http://127.0.0.1:{PORT}/docs\", timeout=1)\n",
        "        if r.status_code == 200:\n",
        "            print(\"Server is up ✅  /docs reachable\")\n",
        "            break\n",
        "    except:\n",
        "        pass\n",
        "    time.sleep(1)\n",
        "\n"
      ],
      "metadata": {
        "id": "1gLsG8Sx3yfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "payload = {\"message\": \"Hi Koa! In one sentence, what can you do?\"}\n",
        "r = requests.post(f\"http://127.0.0.1:{PORT}/chat\", json=payload, timeout=300)\n",
        "\n",
        "print(\"Status:\", r.status_code)\n",
        "print(\"Reply:\", r.json().get(\"reply\"))\n"
      ],
      "metadata": {
        "id": "71GQT_ve3zV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "text = \"\"\"Summarize this:\n",
        "Koa is a machine-learning chat assistant with a FastAPI backend and a responsive web UI.\n",
        "It supports configurable prompts and is built for fast, streaming-style responses.\n",
        "The system is designed for easy local deployment and web use.\n",
        "\"\"\"\n",
        "\n",
        "r = requests.post(f\"http://127.0.0.1:{PORT}/chat\", json={\"message\": text}, timeout=300)\n",
        "print(r.json().get(\"reply\"))\n"
      ],
      "metadata": {
        "id": "VYcnKLFk3zzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"✅ Koa backend is running and responding to /chat on port {PORT}.\")\n"
      ],
      "metadata": {
        "id": "ld6TLOK030o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f \"uvicorn inference.api_server:app\"\n"
      ],
      "metadata": {
        "id": "muPBC2U6Go8D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}